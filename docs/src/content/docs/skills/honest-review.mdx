---
title: "honest-review"
description: "Research-driven code review with confidence-scored, evidence-validated findings. Session review or full codebase audit via parallel teams."
---

{/* HAND-MAINTAINED */}

import { Badge, Tabs, TabItem, Card, CardGrid, LinkCard, Aside, Steps } from '@astrojs/starlight/components';

<Badge text="honest-review" variant="note" /> <Badge text="MIT" variant="success" /> <Badge text="v3.0" variant="success" /> <Badge text="wyattowalsh" variant="caution" /> <Badge text="13/14 patterns" variant="tip" />

> Research-driven code review with confidence-scored, evidence-validated findings. Session review or full codebase audit via parallel teams. Use when reviewing changes, auditing codebases, verifying work quality. NOT for writing new code, explaining code, or benchmarking.

<div class="quick-start">

### Quick Start

**Install:**
```bash
npx skills add wyattowalsh/agents/skills/honest-review -g
```

**Use:** `/honest-review [path | audit | PR#]`

</div>

Works with [Claude Code](https://docs.anthropic.com/en/docs/claude-code), [Gemini CLI](https://github.com/google-gemini/gemini-cli), and other [agentskills.io](https://agentskills.io)-compatible agents.

## What It Does

Research-driven code review where every finding is validated with evidence. The core differentiator is **research validation** -- findings are confirmed with external evidence (Context7, WebSearch, gh) rather than relying solely on LLM knowledge.

<Aside type="caution" title="Core differentiator">
Every non-trivial finding must have research evidence or be discarded. Confidence below 0.3 means discard (except P0/S0 -- report as unconfirmed).
</Aside>

### v3.0 Highlights

- **8 creative lenses** including new Compliance and Dependency lenses
- **Review history** -- save, load, and diff reviews across sessions
- **Dashboard template** -- self-contained HTML findings visualization
- **Degraded mode** -- confidence ceilings when research tools are unavailable
- **Classification gating** -- Light/Standard/Deep review depth based on risk score
- **CI integration** -- GitHub Actions workflow with exit codes and inline PR comments
- **Hooks** -- PreToolUse warns about uncommitted changes, PostToolUse shows git diff stat

## Modes

| $ARGUMENTS | Mode |
|------------|------|
| Empty + changes in session (git diff) | Session review of changed files |
| Empty + no changes (first message) | Full codebase audit |
| File or directory path | Scoped review of that path |
| "audit" | Force full codebase audit |
| PR number/URL | Review PR changes (gh pr diff) |
| Git range (HEAD\~3..HEAD) | Review changes in that range |
| "history" [project] | Show review history for project |
| "diff" or "delta" [project] | Compare current vs. previous review |

## Review Pipeline

Both modes follow a 4-wave pipeline:

<Steps>

1. **Triage (Wave 0)** -- Risk-stratify files as HIGH/MEDIUM/LOW. Run `uv run scripts/project-scanner.py` for project profiling. Compute review depth score (0-10) for classification gating. Determine specialist triggers (security, observability, requirements).

2. **Analysis (Wave 1)** -- Scale by scope: 1-2 files get inline review at all 3 levels; 3-5 files get 3 parallel level-based reviewers (Correctness/Design/Efficiency); 6+ files get a full team with lead, domain reviewers, and specialists. Each reviewer runs 3 internal passes (A: scan, B: deep dive, C: research).

3. **Research Validation (Wave 2)** -- Spawn research subagents to confirm findings with external evidence. Dispatch order: slopsquatting detection first (security-critical), then HIGH-risk findings (2+ sources), then MEDIUM-risk (1 source). Batch 5-8 findings per subagent. In degraded mode, apply confidence ceilings per unavailable tool.

4. **Judge Reconciliation (Wave 3)** -- Normalize findings, cluster by root cause, deduplicate with weighted confidence merging (`1-(1-c1)(1-c2)...`), apply confidence filter, resolve conflicts, check interactions, elevate systemic patterns (3+ files), and rank by score = severity\_weight x confidence x blast\_radius.

</Steps>

## Review Levels

Three abstraction levels, each examining defects and unnecessary complexity:

| Level | Focus | Simplify |
|-------|-------|----------|
| **Correctness** (does it work?) | Error handling, boundary conditions, security, API misuse, concurrency, resource leaks | Phantom error handling, defensive checks for impossible states, dead error paths |
| **Design** (is it well-built?) | Abstraction quality, coupling, cohesion, test quality, cognitive complexity | Dead code, 1:1 wrappers, single-use abstractions, over-engineering |
| **Efficiency** (is it economical?) | Algorithmic complexity, N+1, data structure choice, resource usage, caching | Unnecessary serialization, redundant computation, premature optimization |

Context-dependent triggers activate automatically when relevant: security, observability, AI code smells, config/secrets, resilience, i18n/accessibility, data migration, backward compatibility, infrastructure as code, and requirements validation.

## Creative Lenses

Apply at least 2 lenses per review scope. For security-sensitive code, **Adversary** is mandatory.

- **Inversion** -- assume the code is wrong; what would break first?
- **Deletion** -- remove each unit; does anything else notice?
- **Newcomer** -- read as a first-time contributor; where do you get lost?
- **Incident** -- imagine a 3 AM page; what path led here?
- **Evolution** -- fast-forward 6 months of feature growth; what becomes brittle?
- **Adversary** -- what would an attacker do with this code?
- **Compliance** -- does this code meet regulatory requirements?
- **Dependency** -- is the dependency graph healthy?

## Severity Calibration

Adapts review depth to project type:

| Project Type | Review Depth |
|-------------|-------------|
| **Prototype** | P0/S0 only. Skip style, structure, and optimization concerns. |
| **Production** | Full review at all levels and severities. |
| **Library** | Full review plus backward compatibility focus on public API surfaces. |

## Team Structure (6+ Files)

```
[Lead: triage (Wave 0), Judge reconciliation (Wave 3), final report]
  |-- Correctness Reviewer --> Passes A/B/C internally
  |-- Design Reviewer --> Passes A/B/C internally
  |-- Efficiency Reviewer --> Passes A/B/C internally
  |-- [Security Specialist if triage triggers]
  |-- [Observability Specialist if triage triggers]
  |-- [Requirements Validator if intent available]
```

Each reviewer runs 3 internal passes: Pass A (quick scan, haiku), Pass B (deep dive HIGH-risk files, opus), Pass C (research validate findings).

## State Management

Review history is persisted to `~/.claude/honest-reviews/` via `scripts/review-store.py`:

| Command | Description |
|---------|-------------|
| `save` | Save review findings with project, mode, commit, and scope metadata |
| `load` | Retrieve a specific review (by project and optional date) |
| `list` | List saved reviews with metadata |
| `diff` | Compare two reviews -- shows new, resolved, and recurring findings |

Use `/honest-review history my-project` to view history or `/honest-review diff my-project` to compare against a previous review.

## Dashboard

After Judge reconciliation, findings can be rendered into a self-contained HTML dashboard at `templates/dashboard.html`. Inject the findings JSON into the `<script id="data">` tag. The dashboard auto-detects the view type:

- **Session view** -- findings table with severity/confidence heatmap, strengths, statistics
- **Audit view** -- multi-domain visualization with health radar chart
- **Diff view** -- three-column layout: new (red), resolved (green), recurring (yellow)

## Ecosystem

<Tabs>
  <TabItem label="Scripts">

| Script | Purpose |
|--------|---------|
| `scripts/project-scanner.py` | Wave 0 triage -- deterministic project profiling with adaptive indent detection and weighted risk scoring |
| `scripts/finding-formatter.py` | Wave 3 Judge -- normalize findings to JSON, supports JSON input via `--input`, preserves unknown keys |
| `scripts/review-store.py` | State management -- save, load, list, diff review history |

  </TabItem>
  <TabItem label="References">

| File | When to Read | ~Tokens |
|------|--------------|---------|
| `references/triage-protocol.md` | During Wave 0 triage | 700 |
| `references/checklists.md` | During analysis or building teammate prompts | 1000 |
| `references/research-playbook.md` | When setting up research validation (Wave 2) | 900 |
| `references/judge-protocol.md` | During Judge reconciliation (Wave 3) | 800 |
| `references/output-formats.md` | When producing final output | 900 |
| `references/team-templates.md` | When designing teams | 1200 |
| `references/review-lenses.md` | When applying creative review lenses | 700 |
| `references/ci-integration.md` | When running in CI pipelines | 500 |

  </TabItem>
  <TabItem label="Evals">

7 evaluation scenarios covering all dispatch paths: session review, full audit, finding quality, healthy codebase, PR review, git range, and degraded mode.

  </TabItem>
</Tabs>

## Critical Rules

1. Never skip triage (Wave 0) -- risk classification informs everything downstream
2. Every non-trivial finding must have research evidence or be discarded
3. Confidence \< 0.3 = discard (except P0/S0 -- report as unconfirmed)
4. Do not police style -- follow the codebase's conventions
5. Do not report phantom bugs requiring impossible conditions
6. More than 15 findings means re-prioritize -- 5 validated findings beat 50 speculative
7. Never skip Judge reconciliation (Wave 3)
8. Always present before implementing (approval gate)
9. Always verify after implementing (build, tests, behavior)
10. Never assign overlapping file ownership
11. Maintain positive-to-constructive ratio of 3:1
12. Acknowledge healthy codebases explicitly
13. Apply at least 2 creative lenses per scope -- Adversary mandatory for security code
14. Load ONE reference file at a time
15. Review against the codebase's own conventions, not an ideal standard

---

### General

| Field | Value |
| ----- | ----- |
| Name | `honest-review` |
| License | MIT |
| Version | 3.0 |
| Author | wyattowalsh |
| Audit Score | 100/103 (Grade A) |
| Patterns | 13/14 |

{/* Source of truth: AGENTS.md §6 -- update there first */}
## Supported Agents

| Agent | Reads | Bridge File |
|-------|-------|-------------|
| [Claude Code](https://docs.anthropic.com/en/docs/claude-code) | `CLAUDE.md` | `CLAUDE.md` |
| [Gemini CLI](https://github.com/google-gemini/gemini-cli) | `GEMINI.md` | `GEMINI.md` |
| [Antigravity](https://github.com/google-gemini/gemini-cli) | `GEMINI.md` | `GEMINI.md` |
| [Codex](https://github.com/openai/codex) | `AGENTS.md` | -- |
| [Crush](https://github.com/crush-ai/crush) | `AGENTS.md` | -- |
| [OpenCode](https://github.com/opencode-ai/opencode) | `AGENTS.md` | -- |
| [Cursor](https://cursor.sh) | `AGENTS.md` | -- |
| [GitHub Copilot](https://github.com/features/copilot) | `AGENTS.md` + `CLAUDE.md` + `GEMINI.md` | -- |

<Aside type="tip" title="Cross-platform agents">
The `wagents install` command distributes agent-compatible skills across all supported platforms at once. Run `wagents install -a claude-code -a cursor` to target specific agents.
</Aside>

## Related Skills

<CardGrid>
  <LinkCard title="skill-creator" href="/skills/skill-creator/" description="Create, improve, and audit AI agent skills with 13 proven structural patterns." />
  <LinkCard title="orchestrator" href="/skills/orchestrator/" description="Build and deploy parallel execution via subagent waves, agent teams, and multi-wave pipelines." />
  <LinkCard title="add-badges" href="/skills/add-badges/" description="Scan a codebase to detect languages, frameworks, CI/CD pipelines, package managers, and tools, then generate and insert shields." />
  <LinkCard title="wargame" href="/skills/wargame/" description="Domain-agnostic strategic decision analysis and wargaming with AI-controlled actors." />
</CardGrid>

<details>
<summary>View Full SKILL.md</summary>

````yaml title="SKILL.md"
---
name: honest-review
description: >-
  Research-driven code review with confidence-scored, evidence-validated findings.
  Session review or full codebase audit via parallel teams. Use when reviewing
  changes, auditing codebases, verifying work quality. NOT for writing new code,
  explaining code, or benchmarking.
argument-hint: "[path | audit | PR#]"
license: MIT
metadata:
  author: wyattowalsh
  version: "3.0"
hooks:
  PreToolUse:
    - matcher: Edit
      hooks:
        - command: "bash -c 'if git diff --quiet \"$TOOL_INPUT_file_path\" 2>/dev/null; then exit 0; else echo \"WARNING: $(basename \"$TOOL_INPUT_file_path\") has uncommitted changes\" >&2; exit 0; fi'"
  PostToolUse:
    - matcher: Edit
      hooks:
        - command: "bash -c 'git diff --stat \"$TOOL_INPUT_file_path\" 2>/dev/null || true'"
---

# Honest Review

Research-driven code review. Every finding validated with evidence.
4-wave pipeline: Triage → Analysis → Research → Judge.

**Scope:** Code review and audit only. NOT for writing new code, explaining code, or benchmarking.
````

[Download from GitHub](https://raw.githubusercontent.com/wyattowalsh/agents/main/skills/honest-review/SKILL.md)

</details>

## Resources

<CardGrid>
  <LinkCard title="All Skills" href="/skills/" description="Browse the full skill catalog." />
  <LinkCard title="CLI Reference" href="/cli/" description="Install and manage skills." />
  <LinkCard title="agentskills.io" href="https://agentskills.io" description="The open ecosystem for cross-agent skills." />
</CardGrid>

---
[View source on GitHub](https://github.com/wyattowalsh/agents/blob/main/skills/honest-review/SKILL.md)
