---
title: "honest-review"
description: "Research-driven code review with confidence-scored, evidence-validated findings. Session review or full codebase audit via parallel teams."
---

{/* HAND-MAINTAINED */}

import { Aside, Badge, Card, CardGrid, LinkCard, Steps, Tabs, TabItem } from '@astrojs/starlight/components';

<Badge text="honest-review" variant="note" /> <Badge text="MIT" variant="success" /> <Badge text="v5.0" variant="success" /> <Badge text="wyattowalsh" variant="caution" />

> Research-driven code review with confidence-scored, evidence-validated findings. Session review or full codebase audit via parallel teams. Use when reviewing changes, auditing codebases, verifying work quality. NOT for writing new code, explaining code, or benchmarking.

<div class="quick-start">

### Quick Start

**Install:**
```bash
npx skills add wyattowalsh/agents/skills/honest-review -g
```

**Use:** `/honest-review [path | audit | PR#]`

</div>

Works with [Claude Code](https://docs.anthropic.com/en/docs/claude-code), [Gemini CLI](https://github.com/google-gemini/gemini-cli), and other [agentskills.io](https://agentskills.io)-compatible agents.

## What It Does

Research-driven code review where every finding is validated with evidence. The core differentiator is **research validation** -- findings are confirmed with external evidence (Context7, WebSearch, gh) rather than relying solely on LLM knowledge.

<Aside type="caution" title="Core differentiator">
Every non-trivial finding must have research evidence or be discarded. Confidence below 0.3 means discard (except P0/S0 -- report as unconfirmed).
</Aside>

### v5.0 Highlights

<CardGrid>
  <Card title="Reasoning Chains" icon="document">
    Every finding must explain WHY before stating WHAT. Reduces false positives by 51% (Cubic research).
  </Card>
  <Card title="Citation Anchors" icon="approve-check">
    `[file:start-end]` references mechanically verified against source. Mismatched refs discard the finding.
  </Card>
  <Card title="Agentic Verification" icon="magnifier">
    Three-phase review: Flag, Verify (tool calls), then Validate (research). Grep/Read confirm before reporting.
  </Card>
  <Card title="Multi-Pass Diversity" icon="random">
    3 parallel Pass A subagents with deterministic ordering diversity. Majority voting elevates consensus flags.
  </Card>
  <Card title="Conventional Comments" icon="comment">
    Machine-parseable PR output: `issue (blocking): ...` format for CI annotations and PR comments.
  </Card>
  <Card title="Dependency Context" icon="seti:json">
    Cross-file dependency graph built during triage. High fan-in files auto-elevated to HIGH risk.
  </Card>
  <Card title="Learning Loop" icon="open-book">
    Store false-positive dismissals per project. Similar findings suppressed in future reviews.
  </Card>
  <Card title="OWASP 2025" icon="warning">
    Updated checklists for A03:2025 (Supply Chain) and A10:2025 (Exception Handling).
  </Card>
</CardGrid>

Also includes: 10 creative lenses, review history, HTML dashboard, degraded mode, classification gating, CI integration, and hooks.

## Modes

| $ARGUMENTS | Mode |
|------------|------|
| Empty + changes in session (git diff) | Session review of changed files |
| Empty + no changes (first message) | Full codebase audit |
| File or directory path | Scoped review of that path |
| "audit" | Force full codebase audit |
| PR number/URL | Review PR changes (gh pr diff) |
| Git range (HEAD\~3..HEAD) | Review changes in that range |
| "history" [project] | Show review history for project |
| "diff" or "delta" [project] | Compare current vs. previous review |
| `--format sarif` (with any mode) | Output findings in SARIF v2.1 |
| "learnings" [command] | Manage false-positive learnings (add/list/clear) |
| `--format conventional` (with any mode) | Output in Conventional Comments format |

## Review Pipeline

Both modes follow a 4-wave pipeline:

<Steps>

1. **Triage (Wave 0)** -- Risk-stratify files as HIGH/MEDIUM/LOW. Run `uv run scripts/project-scanner.py` for project profiling. Compute review depth score (0-10) for classification gating. Determine specialist triggers (security, observability, requirements).

2. **Analysis (Wave 1)** -- Scale by scope: 1-2 files get inline review at all 3 levels; 3-5 files get 3 parallel level-based reviewers (Correctness/Design/Efficiency); 6+ files get a full team with lead, domain reviewers, and specialists. Each reviewer runs 3 internal passes (A: scan, B: deep dive, C: research).

3. **Research Validation (Wave 2)** -- Three-phase review: **Flag** (hypothesize), **Verify** (tool calls via Grep/Read to confirm assumptions before reporting), **Validate** (spawn research subagents for external evidence). Dispatch order: slopsquatting detection first, then HIGH-risk (2+ sources), then MEDIUM-risk. In degraded mode, apply confidence ceilings per unavailable tool.

4. **Judge Reconciliation (Wave 3)** -- Normalize findings, cluster by root cause, deduplicate with weighted confidence merging (`1-(1-c1)(1-c2)...`), apply confidence filter, resolve conflicts, check interactions, elevate systemic patterns (3+ files), and rank by score = severity\_weight x confidence x blast\_radius.

</Steps>

## Review Levels

Three abstraction levels, each examining defects and unnecessary complexity:

| Level | Focus | Simplify |
|-------|-------|----------|
| **Correctness** (does it work?) | Error handling, boundary conditions, security, API misuse, concurrency, resource leaks | Phantom error handling, defensive checks for impossible states, dead error paths |
| **Design** (is it well-built?) | Abstraction quality, coupling, cohesion, test quality, cognitive complexity | Dead code, 1:1 wrappers, single-use abstractions, over-engineering |
| **Efficiency** (is it economical?) | Algorithmic complexity, N+1, data structure choice, resource usage, caching | Unnecessary serialization, redundant computation, premature optimization |

Context-dependent triggers activate automatically when relevant: security, observability, AI code smells, config/secrets, resilience, i18n/accessibility, data migration, backward compatibility, infrastructure as code, and requirements validation.

## Creative Lenses

Apply at least 2 lenses per review scope. For security-sensitive code, **Adversary** is mandatory.

- **Inversion** -- assume the code is wrong; what would break first?
- **Deletion** -- remove each unit; does anything else notice?
- **Newcomer** -- read as a first-time contributor; where do you get lost?
- **Incident** -- imagine a 3 AM page; what path led here?
- **Evolution** -- fast-forward 6 months of feature growth; what becomes brittle?
- **Adversary** -- what would an attacker do with this code?
- **Compliance** -- does this code meet regulatory requirements?
- **Dependency** -- is the dependency graph healthy?
- **Cost** -- what does this cost to run?
- **Sustainability** -- will this scale without linear cost growth?

## Finding Structure

Every finding follows this mandatory order:

<Steps>

1. **Citation anchor** -- `[file:start-end]` exact source location, mechanically verified

2. **Reasoning chain** -- WHY this is a problem (written before the finding statement)

3. **Finding statement** -- WHAT the problem is

4. **Evidence** -- external validation source (Context7, WebSearch, gh)

5. **Fix** -- recommended approach

</Steps>

<Aside type="caution" title="Mechanical verification">
Citation anchors are verified against actual source code. If the referenced lines don't contain the described code, the finding is discarded as hallucinated.
</Aside>

## Severity Calibration

Adapts review depth to project type:

| Project Type | Review Depth |
|-------------|-------------|
| **Prototype** | P0/S0 only. Skip style, structure, and optimization concerns. |
| **Production** | Full review at all levels and severities. |
| **Library** | Full review plus backward compatibility focus on public API surfaces. |

## Team Structure (6+ Files)

```
[Lead: triage (Wave 0), Judge reconciliation (Wave 3), final report]
  |-- Correctness Reviewer --> Passes A/B/C internally
  |-- Design Reviewer --> Passes A/B/C internally
  |-- Efficiency Reviewer --> Passes A/B/C internally
  |-- [Security Specialist if triage triggers]
  |-- [Observability Specialist if triage triggers]
  |-- [Requirements Validator if intent available]
```

Each reviewer runs 3 internal passes: Pass A (quick scan, haiku), Pass B (deep dive HIGH-risk files, opus), Pass C (research validate findings).

## State Management

Review history is persisted to `~/.claude/honest-reviews/` via `scripts/review-store.py`:

| Command | Description |
|---------|-------------|
| `save` | Save review findings with project, mode, commit, and scope metadata |
| `load` | Retrieve a specific review (by project and optional date) |
| `list` | List saved reviews with metadata |
| `diff` | Compare two reviews -- shows new, resolved, and recurring findings |

Use `/honest-review history my-project` to view history or `/honest-review diff my-project` to compare against a previous review.

## Dashboard

After Judge reconciliation, findings can be rendered into a self-contained HTML dashboard at `templates/dashboard.html`. Inject the findings JSON into the `<script id="data">` tag. The dashboard auto-detects the view type:

- **Session view** -- findings table with severity/confidence heatmap, strengths, statistics
- **Audit view** -- multi-domain visualization with health radar chart
- **Diff view** -- three-column layout: new (red), resolved (green), recurring (yellow)

## Ecosystem

<Tabs>
  <TabItem label="Scripts">

| Script | Purpose |
|--------|---------|
| `scripts/project-scanner.py` | Wave 0 triage -- project profiling with dependency graph construction and fan-in risk scoring |
| `scripts/finding-formatter.py` | Wave 3 Judge -- normalize findings to JSON, supports `--format sarif` and `--format conventional` |
| `scripts/review-store.py` | State management -- save, load, list, diff review history (schema v2 with reasoning tracking) |
| `scripts/learnings-store.py` | Learning loop -- add, check, list, clear false-positive dismissals per project |
| `scripts/sarif-uploader.py` | Upload SARIF results to GitHub Code Scanning |

  </TabItem>
  <TabItem label="References">

| File | When to Read | ~Tokens |
|------|--------------|---------|
| `references/triage-protocol.md` | Wave 0 triage (incl. dependency graph) | 1500 |
| `references/checklists.md` | Analysis or teammate prompts (incl. OWASP 2025) | 2800 |
| `references/research-playbook.md` | Three-phase research validation (Wave 2) | 2200 |
| `references/judge-protocol.md` | Judge reconciliation (Wave 3, incl. learnings check) | 1200 |
| `references/self-verification.md` | Wave 3.5 -- agentic verification + hallucination detection | 900 |
| `references/output-formats.md` | Final output (reasoning chains + citation anchors) | 1100 |
| `references/team-templates.md` | Team design (multi-pass ordering diversity) | 2200 |
| `references/review-lenses.md` | Creative review lenses (10 lenses) | 1600 |
| `references/ci-integration.md` | CI pipelines (Conventional Comments format) | 700 |
| `references/conventional-comments.md` | PR comments and CI annotations | 400 |
| `references/dependency-context.md` | Cross-file dependency analysis | 500 |
| `references/supply-chain-security.md` | Dependency and build pipeline security | 1000 |
| `references/auto-fix-protocol.md` | Implementing fixes after approval | 800 |
| `references/sarif-output.md` | SARIF format for CI tooling | 700 |

  </TabItem>
  <TabItem label="Evals">

18 evaluation scenarios covering all dispatch paths: session review, full audit, finding quality, healthy codebase, PR review, git range, degraded mode, reasoning chains, agentic verification, conventional comments, dependency context, learnings loop, multi-pass review, and self-verification.

  </TabItem>
</Tabs>

## Critical Rules

1. Never skip triage (Wave 0) -- risk classification informs everything downstream
2. Every non-trivial finding must have research evidence or be discarded
3. Confidence \< 0.3 = discard (except P0/S0 -- report as unconfirmed)
4. Do not police style -- follow the codebase's conventions
5. Do not report phantom bugs requiring impossible conditions
6. More than 12 findings means re-prioritize -- 5 validated findings beat 50 speculative
7. Never skip Judge reconciliation (Wave 3)
8. Always present before implementing (approval gate)
9. Always verify after implementing (build, tests, behavior)
10. Never assign overlapping file ownership
11. Maintain positive-to-constructive ratio of 3:1
12. Acknowledge healthy codebases explicitly
13. Apply at least 2 creative lenses per scope -- Adversary mandatory for security code
14. Load ONE reference file at a time
15. Review against the codebase's own conventions, not an ideal standard
16. Run self-verification (Wave 3.5) when 3+ findings survive Judge
17. Follow auto-fix protocol -- never apply without diff preview and user confirmation
18. Check for convention files (AGENTS.md, CLAUDE.md, .cursorrules) during triage
19. Every finding must include a reasoning chain (WHY) before the finding statement (WHAT)
20. Every finding must include a citation anchor `[file:start-end]` mechanically verified against source
21. Check learnings store during Judge Wave 3 -- suppress findings matching stored false-positive dismissals

---

### General

| Field | Value |
| ----- | ----- |
| Name | `honest-review` |
| License | MIT |
| Version | 5.0 |
| Author | wyattowalsh |

{/* Source of truth: AGENTS.md §6 -- update there first */}
## Supported Agents

| Agent | Reads | Bridge File |
|-------|-------|-------------|
| [Claude Code](https://docs.anthropic.com/en/docs/claude-code) | `CLAUDE.md` | `CLAUDE.md` |
| [Gemini CLI](https://github.com/google-gemini/gemini-cli) | `GEMINI.md` | `GEMINI.md` |
| [Antigravity](https://github.com/google-gemini/gemini-cli) | `GEMINI.md` | `GEMINI.md` |
| [Codex](https://github.com/openai/codex) | `AGENTS.md` | -- |
| [Crush](https://github.com/crush-ai/crush) | `AGENTS.md` | -- |
| [OpenCode](https://github.com/opencode-ai/opencode) | `AGENTS.md` | -- |
| [Cursor](https://cursor.sh) | `AGENTS.md` | -- |
| [GitHub Copilot](https://github.com/features/copilot) | `AGENTS.md` + `CLAUDE.md` + `GEMINI.md` | -- |

<Aside type="tip" title="Cross-platform agents">
The `wagents install` command distributes agent-compatible skills across all supported platforms at once. Run `wagents install -a claude-code -a cursor` to target specific agents.
</Aside>

## Related Skills

<CardGrid>
  <LinkCard title="skill-creator" href="/skills/skill-creator/" description="Create, improve, and audit AI agent skills with 13 proven structural patterns." />
  <LinkCard title="orchestrator" href="/skills/orchestrator/" description="Build and deploy parallel execution via subagent waves, agent teams, and multi-wave pipelines." />
  <LinkCard title="add-badges" href="/skills/add-badges/" description="Scan a codebase to detect languages, frameworks, CI/CD pipelines, package managers, and tools, then generate and insert shields." />
  <LinkCard title="wargame" href="/skills/wargame/" description="Domain-agnostic strategic decision analysis and wargaming with AI-controlled actors." />
</CardGrid>

<details>
<summary>View Full SKILL.md</summary>

````yaml title="SKILL.md"
---
name: honest-review
description: >-
  Research-driven code review with confidence-scored, evidence-validated findings.
  Session review or full codebase audit via parallel teams. Use when reviewing
  changes, auditing codebases, verifying work quality. NOT for writing new code,
  explaining code, or benchmarking.
argument-hint: "[path | audit | PR#]"
license: MIT
metadata:
  author: wyattowalsh
  version: "5.0"
model: sonnet
hooks:
  PreToolUse:
    - matcher: Edit
      hooks:
        - command: "bash -c 'if git diff --quiet \"$TOOL_INPUT_file_path\" 2>/dev/null; then exit 0; else echo \"WARNING: $(basename \"$TOOL_INPUT_file_path\") has uncommitted changes\" >&2; exit 0; fi'"
  PostToolUse:
    - matcher: Edit
      hooks:
        - command: "bash -c 'git diff --stat \"$TOOL_INPUT_file_path\" 2>/dev/null || true'"
---

# Honest Review

Research-driven code review. Every finding validated with evidence.
4-wave pipeline: Triage → Analysis → Research → Judge.

**Scope:** Code review and audit only. NOT for writing new code, explaining code, or benchmarking.
````

[Download from GitHub](https://raw.githubusercontent.com/wyattowalsh/agents/main/skills/honest-review/SKILL.md)

</details>

## Resources

<CardGrid>
  <LinkCard title="All Skills" href="/skills/" description="Browse the full skill catalog." />
  <LinkCard title="CLI Reference" href="/cli/" description="Install and manage skills." />
  <LinkCard title="agentskills.io" href="https://agentskills.io" description="The open ecosystem for cross-agent skills." />
</CardGrid>

---
[View source on GitHub](https://github.com/wyattowalsh/agents/blob/main/skills/honest-review/SKILL.md)
